<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>NLP - Tag - </title>
        <link>https://colinder.github.io/tags/nlp/</link>
        <description>NLP - Tag - </description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 31 May 2021 15:23:45 &#43;0900</lastBuildDate><atom:link href="https://colinder.github.io/tags/nlp/" rel="self" type="application/rss+xml" /><item>
    <title>NAVER AI NOW HyperCLOVA_NLP</title>
    <link>https://colinder.github.io/naver_hyperclova/</link>
    <pubDate>Mon, 31 May 2021 15:23:45 &#43;0900</pubDate>
    <author>Author</author>
    <guid>https://colinder.github.io/naver_hyperclova/</guid>
    <description><![CDATA[NAVER HyperCLOVA의 한국어 모델  2021.05.25 NAVER AI NOW HyperCLOVA의 자연어 전처리 과정의 토근화 방법에 대하여 정리
 ​
데이터 토큰화  자연어 처리를 위한 문장 데이터 토큰화에 대한 NAVER의 처리 방법을 정리해보겠습니다.
말뭉치를 어떻게 구성하고 나눌 것인가에 대한 고민 → 서브워드 토크나이저를 활용. 서브워드는 어떤방식으로 진행할 것인가? → **Byte-Pair Encoding (BPE)**를 사용 그 중에서도 **Morpheme-Aware Byte-Level Byte Pair Encoding(형태소 단위로 분리)**을 사용 Morpheme-Aware Byte-Level Byte Pair Encoding는 많은 메모리를 필요로 하여 말뭉치 데이터의 크기를 최적화하는 것이 필요함.]]></description>
</item></channel>
</rss>
